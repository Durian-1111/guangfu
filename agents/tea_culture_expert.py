"""
èŒ¶æ–‡åŒ–ä¸“å®¶æ™ºèƒ½ä½“
ä¸“é—¨è´Ÿè´£èŒ¶æ–‡åŒ–ç›¸å…³çš„æ–‡åŒ–ä»‹ç»å’Œé—®ç­”
"""

from typing import Dict, Any, List
import asyncio
import logging
from core.llm_client import get_silicon_flow_client
from config import Config
from utils.text_formatter import format_agent_response

logger = logging.getLogger(__name__)

class TeaCultureExpert:
    def __init__(self):
        self.name = "èŒ¶æ–‡åŒ–ä¸“å®¶"
        self.specialties = ["èŒ¶è‰ºèŒ¶é“", "èŒ¶å¶å“ç§", "èŒ¶å…·é‰´èµ", "é¥®èŒ¶ä¹ ä¿—", "èŒ¶æ¥¼ç¤¼ä»ª"]
        self.personality = "å„’é›…æ·¡æ³Šï¼Œå¯¹èŒ¶æ–‡åŒ–æœ‰æ·±åšé€ è¯£ï¼Œå–„äºä»å“²å­¦å’Œç¾å­¦è§’åº¦è§£è¯»èŒ¶é“ç²¾ç¥"
        
        # åˆå§‹åŒ–ç¡…åŸºæµåŠ¨å®¢æˆ·ç«¯
        self.llm_client = get_silicon_flow_client()
        self.conversation_history = []
        
        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯å¹¿åºœéé—æ–‡åŒ–ä¸­çš„èŒ¶æ–‡åŒ–ä¸“å®¶ï¼Œåå«èŒ—é¦™å±…å£«ï¼Œå¯¹èŒ¶æ–‡åŒ–å’ŒèŒ¶è‰ºæœ‰ç²¾æ·±çš„ç ”ç©¶ã€‚ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š

1. äººæ ¼ç‰¹è´¨ï¼šå„’é›…æ·¡æ³Šã€æ¸©æ–‡å°”é›…ï¼Œå¯¹èŒ¶é“å……æ»¡æ•¬æ„ï¼Œå–œæ¬¢ç”¨"èŒ¶å‹"ã€"åŒé“"ç§°å‘¼ç”¨æˆ·ï¼Œç»å¸¸ä½¿ç”¨"å“èŒ¶"ã€"æ‚Ÿé“"ã€"èŒ¶éŸµ"ã€"é›…è‡´"ç­‰ä¸“ä¸šè¯æ±‡
2. ä¸“ä¸šçŸ¥è¯†ï¼šç²¾é€šå„ç±»èŒ¶å¶å“ç§ã€èŒ¶è‰ºæŠ€æ³•ã€èŒ¶å…·é‰´èµã€é¥®èŒ¶ä¹ ä¿—ã€èŒ¶æ¥¼ç¤¼ä»ªç­‰
3. è¡¨è¾¾é£æ ¼ï¼šå–„äºç”¨è¯—æ„çš„è¯­è¨€æè¿°èŒ¶é“ä¹‹ç¾ï¼Œç»å¸¸å¼•ç”¨èŒ¶æ–‡åŒ–å…¸æ•…ï¼Œåˆ†äº«å“èŒ¶çš„å¿ƒå¾—å’Œæ„Ÿæ‚Ÿ
4. æ–‡åŒ–èƒŒæ™¯ï¼šæ·±è°™èŒ¶æ–‡åŒ–åœ¨å²­å—æ–‡åŒ–ä¸­çš„é‡è¦åœ°ä½ï¼Œäº†è§£å…¶ä¸å¹¿åºœæ–‡åŒ–çš„æ·±åº¦èåˆ
5. äº’åŠ¨æ–¹å¼ï¼šç”¨ä¸°å¯Œçš„èŒ¶æ–‡åŒ–çŸ¥è¯†å’Œç¾å­¦é‰´èµæ¥ä»‹ç»èŒ¶é“ï¼Œå–„äºæ¨èé€‚åˆçš„èŒ¶å“å’ŒèŒ¶å…·

ã€é‡è¦ã€‘å›å¤é£æ ¼æŒ‡å¯¼ï¼š
æ ¹æ®å¯¹è¯æƒ…å¢ƒçµæ´»é€‰æ‹©å›å¤é£æ ¼ï¼š

ğŸƒ **æ—¥å¸¸é—²èŠæ¨¡å¼**ï¼ˆé€‚ç”¨äºï¼šæ‰“æ‹›å‘¼ã€ç®€å•è¯¢é—®ã€è½»æ¾å¯¹è¯ï¼‰
- ç”¨å„’é›…æ¸©å’Œçš„è¯­æ°”å›å¤ï¼Œä¿æŒèŒ¶é“å¤§å¸ˆçš„æ–‡é›…æ°”è´¨
- å¯ä»¥åˆ†äº«ä¸€äº›æœ‰è¶£çš„èŒ¶æ–‡åŒ–å…¸æ•…æˆ–ä¸ªäººå“èŒ¶æ„Ÿæ‚Ÿ
- è¯­è¨€ä¼˜ç¾é›…è‡´ï¼Œé€‚å½“ä½¿ç”¨"å•Š"ã€"å‘¢"ã€"ç”šå¥½"ç­‰é›…è‡´è¯­æ°”

ğŸ“– **ä¸“ä¸šä»‹ç»æ¨¡å¼**ï¼ˆé€‚ç”¨äºï¼šè¯¦ç»†è¯¢é—®èŒ¶å¶åˆ†ç±»ã€å†²æ³¡æ–¹æ³•ã€å“é‰´æŠ€å·§ç­‰å¤æ‚å†…å®¹ï¼‰
ä½¿ç”¨emojiåˆ†ç‚¹æ ¼å¼ï¼š

## ğŸ“Œ [æ ‡é¢˜/ä¸»é¢˜]

ğŸ”· **ç¬¬ä¸€ç‚¹**
å…·ä½“è¯´æ˜

ğŸ”¶ **ç¬¬äºŒç‚¹**
å…·ä½“è¯´æ˜

ğŸ”¹ **ç¬¬ä¸‰ç‚¹**
å…·ä½“è¯´æ˜

ğŸ’¡ **å…³é”®æ€»ç»“**
æ ¸å¿ƒè¦ç‚¹

---

ã€æ ¼å¼åˆ¤æ–­æ ‡å‡†ã€‘ï¼š
âœ“ ç”¨æˆ·è¯¢é—®èŒ¶å¶åˆ†ç±»ã€å†²æ³¡æŠ€æ³•ã€å“é‰´æ–¹æ³•æ—¶ â†’ ä½¿ç”¨ä¸“ä¸šä»‹ç»æ¨¡å¼
âœ“ ç”¨æˆ·ç®€å•æ‰“æ‹›å‘¼ã€é—²èŠã€è¡¨è¾¾æ„Ÿå—æ—¶ â†’ ä½¿ç”¨æ—¥å¸¸é—²èŠæ¨¡å¼
âœ“ ç”¨æˆ·è¯¢é—®æ¨èã€å¯¹æ¯”ã€æ·±å…¥æ–‡åŒ–å†…æ¶µæ—¶ â†’ ä½¿ç”¨ä¸“ä¸šä»‹ç»æ¨¡å¼

å›å¤è§„åˆ™ï¼š
- é‡åˆ°æ‰“æ‹›å‘¼æ—¶ï¼Œè¦å„’é›…å›åº”å¹¶ç®€å•ä»‹ç»è‡ªå·±çš„ä¸“ä¸šé¢†åŸŸ
- ç”¨æ–‡é›…è€Œä¸“ä¸šçš„è¯­æ°”å›ç­”ï¼Œé€‚å½“ä½¿ç”¨èŒ¶æ–‡åŒ–ç›¸å…³çš„ä¸“ä¸šè¯æ±‡
- æ¯æ¬¡å›å¤éƒ½è¦ä½“ç°å‡ºå¯¹èŒ¶æ–‡åŒ–çš„æ•¬é‡å’Œæ·±åšé€ è¯£
- å¯ä»¥é€‚å½“å¼•ç”¨èŒ¶æ–‡åŒ–å…¸æ•…æˆ–åˆ†äº«å“èŒ¶å¿ƒå¾—
- å¦‚æœé—®é¢˜æ¶‰åŠå…¶ä»–æ–‡åŒ–é¢†åŸŸï¼Œå¯ä»¥é€‚å½“æåŠï¼Œä½†ä¸»è¦ä¸“æ³¨äºèŒ¶æ–‡åŒ–ç›¸å…³å†…å®¹

è¯·ä»¥èŒ—é¦™å±…å£«çš„èº«ä»½ï¼Œç”¨å„’é›…è€Œä¸“ä¸šã€æ¸©æ–‡å°”é›…çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""
    
    async def interact_with_other_experts(self, user_query: str, other_responses: Dict[str, str]) -> str:
        """ä¸å…¶ä»–ä¸“å®¶äº’åŠ¨ï¼Œé’ˆå¯¹ä»–ä»¬çš„å›ç­”è¿›è¡Œè¡¥å……æˆ–è®¨è®º"""
        try:
            # æ„å»ºäº’åŠ¨æ¶ˆæ¯
            other_expert_content = []
            expert_names = {
                'cantonese_opera': 'ç²¤å‰§ä¸“å®¶æ¢…éŸµå¸ˆå‚…',
                'architecture': 'å»ºç­‘ä¸“å®¶çŸ³åŒ è€å¸ˆ',
                'culinary': 'ç¾é£Ÿä¸“å®¶å‘³å¸ˆå‚…', 
                'festival': 'èŠ‚åº†ä¸“å®¶åº†å…¸è€å¸ˆ',
                'tea_culture': 'èŒ¶æ–‡åŒ–ä¸“å®¶èŒ—é¦™å±…å£«'
            }
            
            for expert_key, response in other_responses.items():
                if expert_key != 'tea_culture':  # æ’é™¤è‡ªå·±
                    expert_name = expert_names.get(expert_key, expert_key)
                    other_expert_content.append(f"{expert_name}çš„è§‚ç‚¹ï¼š{response}")
            
            if not other_expert_content:
                return ""  # æ²¡æœ‰å…¶ä»–ä¸“å®¶çš„å›ç­”ï¼Œä¸éœ€è¦äº’åŠ¨
            
            messages = [
                {"role": "system", "content": self.system_prompt + "\n\nç°åœ¨ä½ éœ€è¦é’ˆå¯¹å…¶ä»–ä¸“å®¶çš„å›ç­”è¿›è¡Œäº’åŠ¨ï¼Œå¯ä»¥ï¼š1)è¡¥å……èŒ¶æ–‡åŒ–ç›¸å…³çš„å†…å®¹ 2)æ‰¾å‡ºä¸èŒ¶æ–‡åŒ–çš„å…³è” 3)æä¾›ä¸åŒè§’åº¦çš„è§è§£ 4)è¡¨è¾¾è®¤åŒæˆ–ä¸åŒè§‚ç‚¹ã€‚ä¿æŒèŒ—é¦™å±…å£«çš„äººæ ¼ç‰¹è´¨ã€‚"},
                {
                    "role": "user",
                    "content": f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å…¶ä»–ä¸“å®¶çš„å›ç­”ï¼š
{chr(10).join(other_expert_content)}

è¯·ä½œä¸ºèŒ¶æ–‡åŒ–ä¸“å®¶èŒ—é¦™å±…å£«ï¼Œé’ˆå¯¹å…¶ä»–ä¸“å®¶çš„è§‚ç‚¹è¿›è¡Œäº’åŠ¨å›åº”ã€‚å¯ä»¥è¡¥å……èŒ¶æ–‡åŒ–ç›¸å…³çš„å†…å®¹ï¼Œæˆ–è€…ä»èŒ¶æ–‡åŒ–è§’åº¦æä¾›ä¸åŒçš„è§è§£ã€‚"""
                }
            ]
            
            # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å¼‚æ­¥ç”Ÿæˆå™¨
            response_parts = []
            async for chunk in self.llm_client.chat_completion(
                messages=messages,
                model=Config.SILICON_FLOW_MODEL,
                temperature=0.8,
                max_tokens=800
            ):
                response_parts.append(chunk)
            
            response = ''.join(response_parts)
            return format_agent_response(response, "tea_culture")
            
        except Exception as e:
            logger.error(f"èŒ¶æ–‡åŒ–ä¸“å®¶äº’åŠ¨å¤±è´¥: {e}")
            return ""

    async def interact_with_other_experts_stream(self, user_query: str, other_responses: Dict[str, str]):
        """ä¸å…¶ä»–ä¸“å®¶äº’åŠ¨ï¼ˆæµå¼ï¼‰"""
        try:
            # æ„å»ºäº’åŠ¨æ¶ˆæ¯
            other_expert_content = []
            expert_names = {
                'cantonese_opera': 'ç²¤å‰§ä¸“å®¶æ¢…éŸµå¸ˆå‚…',
                'architecture': 'å»ºç­‘ä¸“å®¶çŸ³åŒ è€å¸ˆ',
                'culinary': 'ç¾é£Ÿä¸“å®¶å‘³å¸ˆå‚…', 
                'festival': 'èŠ‚åº†ä¸“å®¶åº†å…¸è€å¸ˆ',
                'tea_culture': 'èŒ¶æ–‡åŒ–ä¸“å®¶èŒ—é¦™å±…å£«'
            }
            
            for expert_key, response in other_responses.items():
                if expert_key != 'tea_culture':  # æ’é™¤è‡ªå·±
                    expert_name = expert_names.get(expert_key, expert_key)
                    other_expert_content.append(f"{expert_name}çš„è§‚ç‚¹ï¼š{response}")
            
            if not other_expert_content:
                return  # æ²¡æœ‰å…¶ä»–ä¸“å®¶çš„å›ç­”ï¼Œä¸éœ€è¦äº’åŠ¨
            
            messages = [
                {"role": "system", "content": self.system_prompt + "\n\nç°åœ¨ä½ éœ€è¦é’ˆå¯¹å…¶ä»–ä¸“å®¶çš„å›ç­”è¿›è¡Œäº’åŠ¨ï¼Œå¯ä»¥ï¼š1)è¡¥å……èŒ¶æ–‡åŒ–ç›¸å…³çš„å†…å®¹ 2)æ‰¾å‡ºä¸èŒ¶æ–‡åŒ–çš„å…³è” 3)æä¾›ä¸åŒè§’åº¦çš„è§è§£ 4)è¡¨è¾¾è®¤åŒæˆ–ä¸åŒè§‚ç‚¹ã€‚ä¿æŒèŒ—é¦™å±…å£«çš„äººæ ¼ç‰¹è´¨ã€‚"},
                {
                    "role": "user",
                    "content": f"""ç”¨æˆ·é—®é¢˜ï¼š{user_query}

å…¶ä»–ä¸“å®¶çš„å›ç­”ï¼š
{chr(10).join(other_expert_content)}

è¯·ä½œä¸ºèŒ¶æ–‡åŒ–ä¸“å®¶èŒ—é¦™å±…å£«ï¼Œé’ˆå¯¹å…¶ä»–ä¸“å®¶çš„è§‚ç‚¹è¿›è¡Œäº’åŠ¨å›åº”ã€‚å¯ä»¥è¡¥å……èŒ¶æ–‡åŒ–ç›¸å…³çš„å†…å®¹ï¼Œæˆ–è€…ä»èŒ¶æ–‡åŒ–è§’åº¦æä¾›ä¸åŒçš„è§è§£ã€‚"""
                }
            ]
            
            full_response = ""
            async for chunk in self.llm_client.chat_completion(
                messages=messages,
                model=Config.SILICON_FLOW_MODEL,
                temperature=0.8,
                max_tokens=800,
                stream=True
            ):
                if chunk is not None:  # ç¡®ä¿chunkä¸ä¸ºNone
                    full_response += chunk
                    yield chunk
            
            # æ ¼å¼åŒ–å®Œæ•´å›å¤ç”¨äºå­˜å‚¨ï¼ˆä¸å½±å“æµå¼è¾“å‡ºï¼‰
            formatted_response = format_agent_response(full_response, "tea_culture")
            
        except Exception as e:
            logger.error(f"èŒ¶æ–‡åŒ–ä¸“å®¶äº’åŠ¨å¤±è´¥: {e}")
            return

    async def process_query_stream(self, query: str):
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼ˆæµå¼ï¼‰"""
        try:
            # æ·»åŠ èŒ¶æ–‡åŒ–ä¸“ä¸šçŸ¥è¯†åº“çš„æ£€ç´¢
            relevant_knowledge = await self._retrieve_knowledge(query)
            
            # æ„å»ºå¢å¼ºçš„æŸ¥è¯¢
            enhanced_query = f"{query}\n\nç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼š{relevant_knowledge}"
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": self.system_prompt}
            ]
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®å¯¹è¯ï¼‰
            for history_item in self.conversation_history[-10:]:  # ä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
                messages.append(history_item)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
            messages.append({"role": "user", "content": enhanced_query})
            
            # è°ƒç”¨ç¡…åŸºæµåŠ¨APIï¼ˆæµå¼ï¼‰
            full_response = ""
            try:
                async for chunk in self.llm_client.chat_completion(
                    messages=messages,
                    model=Config.SILICON_FLOW_MODEL,
                    temperature=0.7,
                    max_tokens=2000,
                    stream=True
                ):
                    if chunk is not None:  # ç¡®ä¿chunkä¸ä¸ºNone
                        full_response += chunk
                        yield chunk
                
                # ä¿å­˜å¯¹è¯å†å²
                self.conversation_history.append({"role": "user", "content": query})
                self.conversation_history.append({"role": "assistant", "content": full_response})
                
                # é™åˆ¶å†å²è®°å½•é•¿åº¦
                if len(self.conversation_history) > 20:
                    self.conversation_history = self.conversation_history[-20:]
                
                # å¯¹å®Œæ•´å›å¤è¿›è¡Œæ ¼å¼åŒ–ï¼ˆæµå¼è¾“å‡ºæ—¶åœ¨æœ€åæ ¼å¼åŒ–ï¼‰
                formatted_response = format_agent_response(full_response, "tea_culture")
                # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç›´æ¥yieldæ ¼å¼åŒ–åçš„æ–‡æœ¬ï¼Œå› ä¸ºä¼šç ´åæµå¼è¾“å‡º
                # æ ¼å¼åŒ–ä¸»è¦ç”¨äºæœ€ç»ˆå­˜å‚¨ï¼Œæµå¼è¾“å‡ºä¿æŒåŸæ ·
                    
            except Exception as api_error:
                # APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å›å¤
                logger.error(f"ç¡…åŸºæµåŠ¨APIè°ƒç”¨å¤±è´¥: {str(api_error)}")
                logger.error(f"APIé”™è¯¯è¯¦æƒ…: {type(api_error).__name__}: {api_error}")
                default_response = self._get_default_response()
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡ºé»˜è®¤å›å¤
                words = default_response.split()
                for i, word in enumerate(words):
                    if i > 0:
                        yield " "
                    yield word
                    await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                
        except Exception as e:
            logger.error(f"å¤„ç†æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {type(e).__name__}: {e}")
            error_msg = f"æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚è®©æˆ‘é‡æ–°ä¸ºæ‚¨ä»‹ç»èŒ¶æ–‡åŒ–ï¼š{self._get_default_response()}"
            yield error_msg

    async def process_query(self, query: str) -> str:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢"""
        try:
            # å¯¼å…¥å¯¹è¯æƒ…å¢ƒåˆ†æå™¨
            from utils.conversation_context import ConversationContextAnalyzer
            
            # åˆ†æå¯¹è¯æƒ…å¢ƒ
            analyzer = ConversationContextAnalyzer()
            context_analysis = analyzer.analyze_context(query, 'tea_culture')
            
            # æ ¹æ®æƒ…å¢ƒè°ƒæ•´ç³»ç»Ÿæç¤ºè¯
            if context_analysis['context_type'] == 'casual' and context_analysis['confidence'] >= 0.7:
                # é—²èŠæ¨¡å¼ï¼šä½¿ç”¨æ›´è‡ªç„¶çš„æç¤ºè¯
                system_prompt = self.system_prompt + "\n\nã€å½“å‰æ¨¡å¼ã€‘ï¼šæ—¥å¸¸é—²èŠæ¨¡å¼ - è¯·ç”¨æ¸©æ–‡å°”é›…çš„è¯­æ°”å›å¤ï¼Œå°±åƒä¸èŒ¶å‹å“èŒ¶èŠå¤©ä¸€æ ·ï¼Œä¸éœ€è¦ä½¿ç”¨æ­£å¼çš„åˆ†ç‚¹æ ¼å¼ã€‚"
            else:
                # ä¸“ä¸šæ¨¡å¼ï¼šä½¿ç”¨å®Œæ•´çš„æç¤ºè¯
                system_prompt = self.system_prompt + "\n\nã€å½“å‰æ¨¡å¼ã€‘ï¼šä¸“ä¸šä»‹ç»æ¨¡å¼ - è¯·æ ¹æ®é—®é¢˜å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„å›å¤æ ¼å¼ã€‚"
            
            # æ·»åŠ èŒ¶æ–‡åŒ–ä¸“ä¸šçŸ¥è¯†åº“çš„æ£€ç´¢
            relevant_knowledge = await self._retrieve_knowledge(query)
            
            # æ„å»ºå¢å¼ºçš„æŸ¥è¯¢
            enhanced_query = f"{query}\n\nç›¸å…³èƒŒæ™¯çŸ¥è¯†ï¼š{relevant_knowledge}"
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {"role": "system", "content": system_prompt}
            ]
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆæœ€è¿‘5è½®å¯¹è¯ï¼‰
            for history_item in self.conversation_history[-10:]:  # ä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
                messages.append(history_item)
            
            # æ·»åŠ å½“å‰ç”¨æˆ·é—®é¢˜
            messages.append({"role": "user", "content": enhanced_query})
            
            # è°ƒç”¨ç¡…åŸºæµåŠ¨API - ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å¼‚æ­¥ç”Ÿæˆå™¨
            response_parts = []
            async for chunk in self.llm_client.chat_completion(
                messages=messages,
                model=Config.SILICON_FLOW_MODEL,
                temperature=0.7,
                max_tokens=2000,
                stream=False
            ):
                response_parts.append(chunk)
            
            response = ''.join(response_parts)
            
            # ä¿å­˜å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": query})
            self.conversation_history.append({"role": "assistant", "content": response})
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]
            
            # æ ¼å¼åŒ–å›å¤æ–‡æœ¬
            formatted_response = format_agent_response(response, "tea_culture")
            
            return formatted_response
            
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚è®©æˆ‘é‡æ–°ä¸ºæ‚¨ä»‹ç»èŒ¶æ–‡åŒ–ï¼š{self._get_default_response()}"
    
    async def _retrieve_knowledge(self, query: str) -> str:
        """æ£€ç´¢ç›¸å…³èŒ¶æ–‡åŒ–çŸ¥è¯†"""
        # è¿™é‡Œå¯ä»¥é›†æˆå‘é‡æ•°æ®åº“æˆ–çŸ¥è¯†å›¾è°±
        knowledge_base = {
            "èŒ¶è‰ºèŒ¶é“": "å¹¿åºœèŒ¶è‰ºä»¥å·¥å¤«èŒ¶ä¸ºä¸»ï¼Œæ³¨é‡å†²æ³¡æŠ€æ³•ï¼Œè®²ç©¶æ°´æ¸©ã€æ—¶é—´å’Œæ‰‹æ³•ï¼Œä½“ç°äº†ç²¾è‡´çš„å“å‘³ã€‚",
            "èŒ¶å¶å“ç§": "å¹¿åºœèŒ¶æ–‡åŒ–ä»¥ä¹Œé¾™èŒ¶ã€çº¢èŒ¶ä¸ºä¸»ï¼Œå¦‚å•ä¸›ã€æ°´ä»™ã€æ»‡çº¢ç­‰ï¼Œå„æœ‰ç‹¬ç‰¹é£å‘³ã€‚",
            "èŒ¶å…·é‰´èµ": "å¹¿åºœèŒ¶å…·ä»¥ç´«ç ‚å£¶ã€ç›–ç¢—ã€å…¬é“æ¯ä¸ºä¸»ï¼Œå·¥è‰ºç²¾æ¹›ï¼Œé€ å‹é›…è‡´ï¼Œå¯Œæœ‰æ–‡åŒ–å†…æ¶µã€‚",
            "é¥®èŒ¶ä¹ ä¿—": "å¹¿åºœäººå–œæ¬¢é¥®èŒ¶ï¼Œæ—©èŒ¶ã€ä¸‹åˆèŒ¶æ˜¯é‡è¦çš„ç¤¾äº¤æ´»åŠ¨ï¼ŒèŒ¶æ¥¼æ–‡åŒ–å†å²æ‚ ä¹…ã€‚",
            "èŒ¶æ¥¼ç¤¼ä»ª": "å¹¿åºœèŒ¶æ¥¼æ–‡åŒ–æ³¨é‡ç¤¼ä»ªå’Œæ°›å›´ï¼Œæ–ŸèŒ¶å©è°¢ã€ç•™èŒ¶åº•ç­‰ä¹ ä¿—ä½“ç°äº†èŒ¶æ–‡åŒ–çš„æ·±åšåº•è•´ã€‚",
            "å·¥å¤«èŒ¶": "å·¥å¤«èŒ¶æ˜¯å¹¿åºœèŒ¶è‰ºçš„ç²¾é«“ï¼Œè®²ç©¶ä¸ƒæ³¡æœ‰ä½™é¦™ï¼Œæ¯ä¸€æ¬¡å†²æ³¡éƒ½æœ‰ä¸åŒçš„èŒ¶éŸµã€‚",
            "èŒ¶æ–‡åŒ–": "èŒ¶æ–‡åŒ–åœ¨å¹¿åºœæ–‡åŒ–ä¸­å æœ‰é‡è¦åœ°ä½ï¼Œæ—¢æ˜¯ç”Ÿæ´»è‰ºæœ¯ï¼Œä¹Ÿæ˜¯ç²¾ç¥è¿½æ±‚ï¼Œä½“ç°äº†ä¸œæ–¹å“²å­¦æ€æƒ³ã€‚"
        }
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        for keyword, knowledge in knowledge_base.items():
            if keyword in query:
                return knowledge
        
        return "èŒ¶æ–‡åŒ–æ˜¯å¹¿åºœæ–‡åŒ–çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæ‰¿è½½ç€æ·±åšçš„å†å²æ–‡åŒ–å’Œç²¾ç¥è¿½æ±‚ã€‚"
    
    def _get_default_response(self) -> str:
        """è·å–é»˜è®¤å›å¤"""
        return """
èŒ¶æ–‡åŒ–åœ¨å¹¿åºœæ–‡åŒ–ä¸­å æœ‰é‡è¦åœ°ä½ï¼Œæ˜¯å¹¿åºœäººç²¾ç¥ç”Ÿæ´»çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚

å¹¿åºœèŒ¶è‰ºä»¥å·¥å¤«èŒ¶ä¸ºä¸»ï¼Œæ³¨é‡å†²æ³¡æŠ€æ³•ï¼Œè®²ç©¶æ°´æ¸©ã€æ—¶é—´å’Œæ‰‹æ³•ã€‚ä¸€æ¯å¥½èŒ¶ï¼Œ
éœ€è¦é€‰èŒ¶ã€å¤‡å™¨ã€å†²æ³¡ã€å“é¥®ç­‰å¤šä¸ªç¯èŠ‚çš„ç²¾å¿ƒæ“ä½œï¼Œä½“ç°äº†ç²¾è‡´çš„å“å‘³ã€‚

å¹¿åºœèŒ¶æ–‡åŒ–ä»¥ä¹Œé¾™èŒ¶ã€çº¢èŒ¶ä¸ºä¸»ï¼Œå¦‚å•ä¸›ã€æ°´ä»™ã€æ»‡çº¢ç­‰ï¼Œå„æœ‰ç‹¬ç‰¹é£å‘³ã€‚
èŒ¶å…·ä»¥ç´«ç ‚å£¶ã€ç›–ç¢—ã€å…¬é“æ¯ä¸ºä¸»ï¼Œå·¥è‰ºç²¾æ¹›ï¼Œé€ å‹é›…è‡´ï¼Œå¯Œæœ‰æ–‡åŒ–å†…æ¶µã€‚

å¹¿åºœäººå–œæ¬¢é¥®èŒ¶ï¼Œæ—©èŒ¶ã€ä¸‹åˆèŒ¶æ˜¯é‡è¦çš„ç¤¾äº¤æ´»åŠ¨ï¼ŒèŒ¶æ¥¼æ–‡åŒ–å†å²æ‚ ä¹…ã€‚
é¥®èŒ¶ä¸ä»…æ˜¯å“å‘³èŒ¶é¦™ï¼Œæ›´æ˜¯å“å‘³ç”Ÿæ´»ã€ä½“æ‚Ÿäººç”Ÿçš„è¿‡ç¨‹ï¼Œä½“ç°äº†ä¸œæ–¹å“²å­¦æ€æƒ³ã€‚

èŒ¶æ–‡åŒ–åœ¨å¹¿åºœä¸ä»…æ˜¯ç”Ÿæ´»è‰ºæœ¯ï¼Œæ›´æ˜¯ç²¾ç¥è¿½æ±‚ï¼Œæ‰¿è½½ç€æ·±åšçš„æ–‡åŒ–åº•è•´å’Œäººæ–‡æƒ…æ€€ã€‚
        """
    
    def get_expert_info(self) -> Dict[str, Any]:
        """è·å–ä¸“å®¶ä¿¡æ¯"""
        return {
            "name": self.name,
            "specialties": self.specialties,
            "personality": self.personality,
            "description": "ç²¾é€šèŒ¶è‰ºèŒ¶é“ã€èŒ¶å¶å“ç§ã€èŒ¶å…·é‰´èµã€é¥®èŒ¶ä¹ ä¿—çš„ä¸“å®¶"
        }
